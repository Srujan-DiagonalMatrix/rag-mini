##### This is a sample RAG Project for the beginners. #####

##### These are the steps to follow in setting up the project: #####
1. Setup Project folder, .venv, install libraries/packages, git, .env
2. create project structure
3. pull ollama embeddings, update OpenAI library.

##### Ollama API for embeddings: #####
curl -s http://localhost:11434/api/tags | head
Instance: http://localhost:11434

#### Target folder structure (minimal additions) #####
app/
â”œâ”€â”€ embeddings.py âœ… already
â”œâ”€â”€ ingestion.py âœ… already (minor fix: splitDocuments returns chunks)
â”œâ”€â”€ llm.py âš  expand for OpenAI chat
â”œâ”€â”€ main.py âŒ CLI entry
â”œâ”€â”€ rag_pipeline.py âŒ orchestrator â€œbrainâ€
â”œâ”€â”€ retrieval.py âŒ NEW: query-time retrieval only
â”œâ”€â”€ prompts.py âŒ NEW: prompt templates/builders
â”œâ”€â”€ generation.py âŒ NEW: LLM answer generation only
â”œâ”€â”€ api.py âŒ NEW: FastAPI app
â”œâ”€â”€ schemas.py âŒ NEW: Pydantic request/response models
â”œâ”€â”€ config.py âŒ NEW: env/config handling
â”œâ”€â”€ logging_utils.py âŒ NEW: logging + request id + timing helpers
â””â”€â”€ vectorstore.py âœ… already

(You can keep fewer files, but this split makes each module single-purpose.)


##### ğŸ¯ WHAT YOU SHOULD BUILD NEXT (Simple Order) #####

Follow this exact order:

Step 2 â†’ Implement Retrieval in rag_pipeline.py
Build: retrieve(question)
Test it alone.

Step 3 â†’ Implement Generation
Add:
generate_answer()
Test it with fake context first.

Step 4 â†’ Create Unified Function
ask(question)
Test end-to-end.

Step 5 â†’ Add CLI
Use argparse.

Step 6 â†’ Add FastAPI
Only after CLI works.